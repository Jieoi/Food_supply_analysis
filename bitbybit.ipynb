{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+9HMI5XHt2X/XqopFK1h2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jieoi/Food_supply_analysis/blob/master/bitbybit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import time"
      ],
      "metadata": {
        "id": "7sQGXC-wyzjk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple CNN model\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Data splitting function: Split data into random subsets\n",
        "def split_data(x_train, y_train, subset_fraction=0.1, num_steps=5):\n",
        "    subsets = []\n",
        "    for step in range(num_steps):\n",
        "        # Select a random subset of data\n",
        "        x_train_subset, y_train_subset = shuffle(x_train, y_train, random_state=step)\n",
        "        subset_size = int(len(x_train_subset) * subset_fraction)\n",
        "        x_train_subset = x_train_subset[:subset_size]\n",
        "        y_train_subset = y_train_subset[:subset_size]\n",
        "        subsets.append((x_train_subset, y_train_subset))\n",
        "        print(f\"Subset {step + 1}/{num_steps} - Size: {subset_size} images\")\n",
        "    return subsets\n",
        "\n",
        "# Reusable model training function: Train the model on a given dataset\n",
        "def train_model_on_data(model, x_train_data, y_train_data, epochs=5):\n",
        "    model.fit(x_train_data, y_train_data, epochs=epochs, batch_size=32, verbose=0)\n",
        "\n",
        "# Model averaging function: Average model weights after training\n",
        "def average_model_weights(model, model_weights):\n",
        "    new_weights = model.get_weights()\n",
        "    averaged_weights = [(w1 + w2) / 2 for w1, w2 in zip(model_weights, new_weights)]\n",
        "    model.set_weights(averaged_weights)\n",
        "    return model.get_weights()\n",
        "\n",
        "# # Function to train and average models on random subsets\n",
        "# def train_and_average_models(x_train, y_train, model, x_test, y_test, num_steps=5, subset_fraction=0.1):\n",
        "#     model_weights = model.get_weights()\n",
        "#     total_start_time = time.time()  # Track total time for this function\n",
        "#     small_models = []  # To store the small models trained on each subset\n",
        "\n",
        "#     # Split data into random subsets\n",
        "#     subsets = split_data(x_train, y_train, subset_fraction, num_steps)\n",
        "\n",
        "#     for step, (x_train_subset, y_train_subset) in enumerate(subsets):\n",
        "#         start_time = time.time()  # Track time for each step\n",
        "#         print(f\"Training step {step + 1}/{num_steps}\")\n",
        "\n",
        "#         # Train the model on this subset\n",
        "#         train_model_on_data(model, x_train_subset, y_train_subset)\n",
        "\n",
        "#         # Average the model weights\n",
        "#         model_weights = average_model_weights(model, model_weights)\n",
        "\n",
        "#         # Store the trained model for this subset\n",
        "#         small_models.append(model_weights)\n",
        "\n",
        "#         # Test the model on the test set after training on the current subset\n",
        "#         test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "#         print(f\"Test accuracy after step {step + 1}: {test_acc:.4f}\")\n",
        "\n",
        "#         step_time = time.time() - start_time\n",
        "#         print(f\"Time taken for step {step + 1}: {step_time:.2f} seconds\")\n",
        "\n",
        "#     total_time = time.time() - total_start_time\n",
        "#     print(f\"\\nTotal time for training and averaging models: {total_time:.2f} seconds\")\n",
        "\n",
        "#     return model, small_models\n",
        "#  train and average models on random subsets\n",
        "def train_and_average_models(x_train, y_train, model, x_test, y_test, num_steps=5, subset_fraction=0.1):\n",
        "    model_weights = model.get_weights()\n",
        "    total_start_time = time.time()  # Track total time for this function\n",
        "    small_models = []  # To store the small models trained on each subset\n",
        "    small_model_accuracies = []  # To store the accuracy of each small model\n",
        "\n",
        "    # Split data into random subsets\n",
        "    subsets = split_data(x_train, y_train, subset_fraction, num_steps)\n",
        "\n",
        "    for step, (x_train_subset, y_train_subset) in enumerate(subsets):\n",
        "        # Start time for training and averaging (exclude testing time)\n",
        "        step_start_time = time.time()\n",
        "        print(f\"Training step {step + 1}/{num_steps}\")\n",
        "\n",
        "        # Train the model on this subset\n",
        "        train_model_on_data(model, x_train_subset, y_train_subset)\n",
        "\n",
        "        # Test accuracy of this small model on the test set (no timing here)\n",
        "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        small_model_accuracies.append(test_acc)\n",
        "        print(f\"Accuracy of the small model trained on subset {step + 1}: {test_acc:.4f}\")\n",
        "\n",
        "        if step > 1:\n",
        "          # Average the model weights\n",
        "          model_weights = average_model_weights(model, model_weights)\n",
        "\n",
        "        # Test the model after averaging weights (accuracy of the current model after including the new model)\n",
        "        test_loss_after_averaging, test_acc_after_averaging = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print(f\"Accuracy of the model after including new model (step {step + 1}): {test_acc_after_averaging:.4f}\")\n",
        "\n",
        "        # Store the trained model weights for this subset\n",
        "        small_models.append(model_weights)\n",
        "\n",
        "        # Time taken for training and averaging (exclude testing time)\n",
        "        step_time = time.time() - step_start_time\n",
        "        print(f\"Time taken for training and averaging step {step + 1}: {step_time:.2f} seconds\")\n",
        "\n",
        "    total_time = time.time() - total_start_time\n",
        "    print(f\"\\nTotal time for training and averaging models (excluding testing): {total_time:.2f} seconds\")\n",
        "\n",
        "    # Return the final model, small models, and their accuracies\n",
        "    return model, small_models, small_model_accuracies\n",
        "\n",
        "\n",
        "\n",
        "# Control model - Train on the entire dataset at once\n",
        "def train_control_model(x_train, y_train, x_test, y_test, epochs=5):\n",
        "    control_model = create_model()\n",
        "    start_time = time.time()  # Track time for control model\n",
        "\n",
        "    # Train the model on the full dataset\n",
        "    train_model_on_data(control_model, x_train, y_train, epochs)\n",
        "\n",
        "    # Evaluate the control model on the test set\n",
        "    test_loss, test_acc = control_model.evaluate(x_test, y_test)\n",
        "    print(f\"Control model accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    control_time = time.time() - start_time\n",
        "    print(f\"Time taken to train control model: {control_time:.2f} seconds\")\n",
        "\n",
        "    return control_model"
      ],
      "metadata": {
        "id": "dGns8_jPy0R-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess data (normalize and reshape)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)  # channel dimension\n",
        "x_test = np.expand_dims(x_test, -1)"
      ],
      "metadata": {
        "id": "Yb-EAb3juCTi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = create_model()\n",
        "\n",
        "# Train and average models on random subsets\n",
        "print(\"\\nTraining on random subsets and averaging models...\")\n",
        "trained_model, small_models, small_model_accuracies = train_and_average_models(x_train, y_train, model, x_test, y_test, num_steps=5, subset_fraction=0.1)\n",
        "\n",
        "# Final evaluation of the model trained on random subsets\n",
        "test_loss, test_acc = trained_model.evaluate(x_test, y_test)\n",
        "print(f\"Final model accuracy (random subsets averaging): {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiqPkrrqzA0e",
        "outputId": "c740b6e2-5125-4aa5-c0a6-10e03a233275"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on random subsets and averaging models...\n",
            "Subset 1/5 - Size: 6000 images\n",
            "Subset 2/5 - Size: 6000 images\n",
            "Subset 3/5 - Size: 6000 images\n",
            "Subset 4/5 - Size: 6000 images\n",
            "Subset 5/5 - Size: 6000 images\n",
            "Training step 1/5\n",
            "Accuracy of the small model trained on subset 1: 0.9703\n",
            "Accuracy of the model after including new model (step 1): 0.9703\n",
            "Time taken for training and averaging step 1: 38.31 seconds\n",
            "Training step 2/5\n",
            "Accuracy of the small model trained on subset 2: 0.9817\n",
            "Accuracy of the model after including new model (step 2): 0.9817\n",
            "Time taken for training and averaging step 2: 30.12 seconds\n",
            "Training step 3/5\n",
            "Accuracy of the small model trained on subset 3: 0.9792\n",
            "Accuracy of the model after including new model (step 3): 0.9720\n",
            "Time taken for training and averaging step 3: 39.88 seconds\n",
            "Training step 4/5\n",
            "Accuracy of the small model trained on subset 4: 0.9823\n",
            "Accuracy of the model after including new model (step 4): 0.9818\n",
            "Time taken for training and averaging step 4: 30.57 seconds\n",
            "Training step 5/5\n",
            "Accuracy of the small model trained on subset 5: 0.9825\n",
            "Accuracy of the model after including new model (step 5): 0.9853\n",
            "Time taken for training and averaging step 5: 29.53 seconds\n",
            "\n",
            "Total time for training and averaging models (excluding testing): 169.84 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9821 - loss: 0.0598\n",
            "Final model accuracy (random subsets averaging): 0.9853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the control model on the full dataset\n",
        "print(\"\\nTraining control model on the full dataset...\")\n",
        "control_model = train_control_model(x_train, y_train, x_test, y_test, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inSEu1-TzJlA",
        "outputId": "3ea7d667-bce5-4d72-e43b-a50f17152af8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training control model on the full dataset...\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.9851 - loss: 0.0424\n",
            "Control model accuracy: 0.9884\n",
            "Time taken to train control model: 314.12 seconds\n"
          ]
        }
      ]
    }
  ]
}